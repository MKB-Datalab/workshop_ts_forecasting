{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f226206c",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages\" data-toc-modified-id=\"Import-Packages-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Packages</a></span></li><li><span><a href=\"#Function-to-help-avoiding-some-memories-issues\" data-toc-modified-id=\"Function-to-help-avoiding-some-memories-issues-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Function to help avoiding some memories issues</a></span></li><li><span><a href=\"#Stationarity-Tests-Function\" data-toc-modified-id=\"Stationarity-Tests-Function-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Stationarity Tests Function</a></span></li><li><span><a href=\"#EDA---The-Walmart-Dataset\" data-toc-modified-id=\"EDA---The-Walmart-Dataset-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>EDA - The Walmart Dataset</a></span></li><li><span><a href=\"#Box-Jenkins-Method\" data-toc-modified-id=\"Box-Jenkins-Method-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Box-Jenkins Method</a></span><ul class=\"toc-item\"><li><span><a href=\"#STEP-1---Identify\" data-toc-modified-id=\"STEP-1---Identify-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>STEP 1 - Identify</a></span><ul class=\"toc-item\"><li><span><a href=\"#Visualize-times-series-sales-of-item-28-at-store-2\" data-toc-modified-id=\"Visualize-times-series-sales-of-item-28-at-store-2-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Visualize times series sales of item 28 at store 2</a></span></li><li><span><a href=\"#Apply-Stationarity-Tests\" data-toc-modified-id=\"Apply-Stationarity-Tests-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;</span>Apply Stationarity Tests</a></span></li><li><span><a href=\"#Plot-ACF-and-PACF\" data-toc-modified-id=\"Plot-ACF-and-PACF-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;</span>Plot ACF and PACF</a></span></li></ul></li><li><span><a href=\"#STEP-2:-Estimate-Coefficients-(p,q)\" data-toc-modified-id=\"STEP-2:-Estimate-Coefficients-(p,q)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>STEP 2: Estimate Coefficients (p,q)</a></span></li><li><span><a href=\"#First-ARIMA-model-(just-an-example)\" data-toc-modified-id=\"First-ARIMA-model-(just-an-example)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>First ARIMA model (just an example)</a></span></li><li><span><a href=\"#STEP-3:-Model-Evaluation\" data-toc-modified-id=\"STEP-3:-Model-Evaluation-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>STEP 3: Model Evaluation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Mean-Absolute-Error-(MAE)\" data-toc-modified-id=\"Mean-Absolute-Error-(MAE)-5.4.1\"><span class=\"toc-item-num\">5.4.1&nbsp;&nbsp;</span>Mean Absolute Error (MAE)</a></span></li><li><span><a href=\"#Diagnostic-Summary-Statistics\" data-toc-modified-id=\"Diagnostic-Summary-Statistics-5.4.2\"><span class=\"toc-item-num\">5.4.2&nbsp;&nbsp;</span>Diagnostic Summary Statistics</a></span></li><li><span><a href=\"#Plot-Diagnostics\" data-toc-modified-id=\"Plot-Diagnostics-5.4.3\"><span class=\"toc-item-num\">5.4.3&nbsp;&nbsp;</span>Plot Diagnostics</a></span></li></ul></li></ul></li><li><span><a href=\"#SARIMA-model\" data-toc-modified-id=\"SARIMA-model-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>SARIMA model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Seasonal-Differencing\" data-toc-modified-id=\"Seasonal-Differencing-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Seasonal Differencing</a></span></li><li><span><a href=\"#Automated-Model-Selection\" data-toc-modified-id=\"Automated-Model-Selection-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Automated Model Selection</a></span></li><li><span><a href=\"#Fitting-and-Evaluating-Best-Model\" data-toc-modified-id=\"Fitting-and-Evaluating-Best-Model-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Fitting and Evaluating Best Model</a></span></li></ul></li><li><span><a href=\"#Forecasting\" data-toc-modified-id=\"Forecasting-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Forecasting</a></span><ul class=\"toc-item\"><li><span><a href=\"#SARIMA-vs-ARIMA-forecasts\" data-toc-modified-id=\"SARIMA-vs-ARIMA-forecasts-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>SARIMA vs ARIMA forecasts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Forecast-in-Sample\" data-toc-modified-id=\"Forecast-in-Sample-7.1.1\"><span class=\"toc-item-num\">7.1.1&nbsp;&nbsp;</span>Forecast in Sample</a></span></li><li><span><a href=\"#Metrics-Used-to-Compare-Models\" data-toc-modified-id=\"Metrics-Used-to-Compare-Models-7.1.2\"><span class=\"toc-item-num\">7.1.2&nbsp;&nbsp;</span>Metrics Used to Compare Models</a></span></li><li><span><a href=\"#Forecast-Out-of-Sample\" data-toc-modified-id=\"Forecast-Out-of-Sample-7.1.3\"><span class=\"toc-item-num\">7.1.3&nbsp;&nbsp;</span>Forecast Out of Sample</a></span></li></ul></li></ul></li><li><span><a href=\"#Saving-Model\" data-toc-modified-id=\"Saving-Model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Saving Model</a></span></li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Conclusions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017b8ddd",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5e66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning, InterpolationWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)\n",
    "warnings.simplefilter('ignore', InterpolationWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6719540",
   "metadata": {},
   "source": [
    "# Function to help avoiding some memories issues\n",
    "\n",
    "I came across the function below at Kaggle but you can find in many different places in the Internet (e.g., https://forum.numer.ai/t/reducing-memory/313). It helps overcoming some memory issues, especially when working with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0509d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use it to decrease memory use\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: \n",
    "        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f25917",
   "metadata": {},
   "source": [
    "# Stationarity Tests Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be1fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "def obtain_adf_kpss_results(timeseries, max_d):\n",
    "    \"\"\" Build dataframe with ADF statistics and p-value for time series after applying difference on time series\n",
    "    \n",
    "    Args:\n",
    "        time_series (df): Dataframe of univariate time series  \n",
    "        max_d (int): Max value of how many times apply difference\n",
    "        \n",
    "    Return:\n",
    "        Dataframe showing values of ADF statistics and p when applying ADF test after applying d times \n",
    "        differencing on a time-series.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    results=[]\n",
    "\n",
    "    for idx in range(max_d):\n",
    "        adf_result = adfuller(timeseries, autolag='AIC')\n",
    "        kpss_result = kpss(timeseries, regression='c', nlags=\"auto\")\n",
    "        timeseries = timeseries.diff().dropna()\n",
    "        if adf_result[1] <=0.05:\n",
    "            adf_stationary = True\n",
    "        else:\n",
    "            adf_stationary = False\n",
    "        if kpss_result[1] <=0.05:\n",
    "            kpss_stationary = False\n",
    "        else:\n",
    "            kpss_stationary = True\n",
    "            \n",
    "        stationary = adf_stationary & kpss_stationary\n",
    "            \n",
    "        results.append((idx,adf_result[1], kpss_result[1],adf_stationary,kpss_stationary, stationary))\n",
    "    \n",
    "    # Construct DataFrame \n",
    "    results_df = pd.DataFrame(results, columns=['d','adf_stats','p-value', 'is_adf_stationary','is_kpss_stationary','is_stationary' ])\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3b2293",
   "metadata": {},
   "source": [
    "# EDA - The Walmart Dataset\n",
    "\n",
    "The data used from now on comes from [**Store Item Demand Forecasting Challenge**](https://www.kaggle.com/c/demand-forecasting-kernels-only) Kaggle competition.\n",
    "\n",
    "The available [`data`](https://www.kaggle.com/c/demand-forecasting-kernels-only/data) consists of 5 years of store-item sales data split in a training dataset (train.csv) and a test dataset (test.csv). The objective of this competition was to forecast 3 months of sales for 50 different items at 10 different stores using the 5 years history of sales.\n",
    "\n",
    "Our goal here is to use part of this data to apply what we have learnt about time series and the forecast methods introduced during this tutorial.\n",
    "\n",
    "First of all, let's get to know our data.\n",
    "\n",
    "**You just need to run the cells in this section, observe and analyze the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5d2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/store_item_demand_forecasting/train.csv\")\n",
    "reduce_mem_usage(df_train)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24be9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37433e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6914b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../data/store_item_demand_forecasting/test.csv\")\n",
    "reduce_mem_usage(df_test)\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e25e321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7eacc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0c67d3",
   "metadata": {},
   "source": [
    "The test dataset was provided by Kaggle to serve as a guide for the submission. It covers the 3 first months of 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300a743",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.groupby('store').nunique()['item']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76474f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby('store').nunique()['item']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fe82a2",
   "metadata": {},
   "source": [
    "As we can see, both train and test datasets have 10 stores and each store offers 50 unique items.\n",
    "`sales` is our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1906f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change dtype of date to datetime\n",
    "df_train['date'] = pd.to_datetime(df_train['date'])\n",
    "df_test['date'] = pd.to_datetime(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06d7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Period covered in train dataset:\", df_train['date'].min(),df_train['date'].max())\n",
    "print(\"Period covered in test dataset:\", df_test['date'].min(),df_test['date'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb89d1b",
   "metadata": {},
   "source": [
    "The training dataset has data from January 1st, 2013 until December 31st, 2017. The goal is to predict sales of items in all stores from January 1st, 2018 until March 31st, 2018, i.e., 3 months.\n",
    "\n",
    "Observe that the dataset consists of multiple time series, one for each store-item, i.e.,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b86a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of time series and lengths\n",
    "print(\"Number of stores: {}.\".format(len(df_train.groupby([\"store\"]).groups.keys())))\n",
    "print(\"Number of items: {}.\".format(len(df_train.groupby([\"item\"]).groups.keys())))\n",
    "print(\"Number of time series: {}.\".format(len(df_train.groupby([\"store\", \"item\"]).groups.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8dc35a",
   "metadata": {},
   "source": [
    "Let's explore and get to know a bit more our dataset. Starting by checking how much each store sells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a8fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['store','sales']].groupby('store').sum().sort_values('sales', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eec6712",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Number of sales per store.\")\n",
    "sns.barplot(data=df_train,x='store',y='sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992019b1",
   "metadata": {},
   "source": [
    "As we can see, store 2 has the highest volume of sales. Which products are the most sold there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a941aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2 = df_train[df_train['store']==2]\n",
    "\n",
    "reduce_mem_usage(df_store_2)\n",
    "\n",
    "df_store_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3e048",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2 = df_store_2[['date','item','sales']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51557a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2[['item','sales']].groupby('item').sum().sort_values('sales', ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5724905",
   "metadata": {},
   "source": [
    "The item with higher sales is item 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac96adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.title(\"Number of sales per item in Store 2.\")\n",
    "sns.barplot(data=df_store_2[['item','sales']].groupby('item').sum().reset_index(),x='item',y='sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fab59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create some time related columns\n",
    "\n",
    "df_store_2['year'] = df_store_2['date'].dt.year\n",
    "df_store_2['month'] = df_store_2['date'].dt.month\n",
    "df_store_2['day'] = df_store_2['date'].dt.dayofyear\n",
    "df_store_2['weekday'] = df_store_2['date'].dt.weekday\n",
    "df_store_2['year-month'] = df_store_2['date'].apply(lambda x: str(x.year)+'-'+str(x.month))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb080097",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Sales per week day.\")\n",
    "sns.boxplot(x=\"weekday\", y=\"sales\", data=df_store_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36614d",
   "metadata": {},
   "source": [
    "Higher sales occur during the weekend (5=Saturday, 6=Sunday). All days present some outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3abe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Sales per month.\")\n",
    "sns.boxplot(x=\"month\", y=\"sales\", data=df_store_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e178fa3c",
   "metadata": {},
   "source": [
    "Higher volume in sales is achieved in July and lowest in January."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef5c25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,4))\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Sales per Year-Month')\n",
    "sns.boxplot(x=\"year-month\", y=\"sales\", data=df_store_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58397196",
   "metadata": {},
   "source": [
    "The pattern of sales per month seems to be pretty the same every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1ca08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_store_2, \n",
    "             x='month', \n",
    "             y='sales', \n",
    "             hue='year', \n",
    "             legend='full',\n",
    "            palette='Dark2')\n",
    "\n",
    "# add title\n",
    "plt.title('Year seasonality plot')\n",
    "\n",
    "# move the legend outside of the main figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f457b27b",
   "metadata": {},
   "source": [
    "The behavior of sales is pretty much the same every year with higher number of sales in July. Furthermore, it show us that the volume of sales is increasing every year. Interesting, to notice that year 2014 and 2015 seem to be close to each other in the number of sales. The same is observed with 2016 and 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b73a99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_store_2, \n",
    "             x='weekday', \n",
    "             y='sales', \n",
    "             hue='month', \n",
    "             legend='full',\n",
    "            palette='Dark2')\n",
    "\n",
    "# add title\n",
    "plt.title('Weekday seasonality plot - per month')\n",
    "\n",
    "# move the legend outside of the main figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b271102",
   "metadata": {},
   "source": [
    "This plot confirms the higher value of sales during the weekends and also July as the month with more sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44db6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=df_store_2, \n",
    "             x='weekday', \n",
    "             y='sales', \n",
    "             hue='year', \n",
    "             legend='full',\n",
    "            palette='Dark2')\n",
    "\n",
    "# add title\n",
    "plt.title('Weekday seasonality plot - per year')\n",
    "\n",
    "# move the legend outside of the main figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd8e65f",
   "metadata": {},
   "source": [
    "Here we also verify higher number of sales during the weekend. In addition, we confirm what was observed on the year seasonality plot: Increase in sales every year with years 2014 and 2015 close in number of sales as well as year 2016 and 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3ebdcb",
   "metadata": {},
   "source": [
    "# Box-Jenkins Method\n",
    "\n",
    "To learn applying (S)ARIMA(X) models we will follow some steps based on [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method). This popular framework provides a systematic way that involves getting to know your data and applying the appropriate methods to choose parameters that will lead to a good model.\n",
    "\n",
    "![](../images/Box-Jenkins-Method.PNG)\n",
    "\n",
    "<!-- ![](https://github.com/MKB-Datalab/time-series-analysis-with-SARIMAX-and-Prophet/blob/master/images/Box-Jenkins-Method.PNG) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a9729",
   "metadata": {},
   "source": [
    "Visualize times series sales of item 28 at store 2\n",
    "\n",
    "As we saw previously in this data, we have 500 time series which are defined by the pair store-item. Here we are working with forecasting individual time series. To forecast sales for all stores and all items we need to apply a forecast model to each one of the time series. \n",
    "\n",
    "With purpose of demonstration, in this tutorial we will work with only one time series: sales of item 28 (the most sold item) at store 2 (the store with the highest number of sales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4785511a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from item 28 store 2\n",
    "\n",
    "df_store_2_item_28 = pd.read_csv(#YOUR CODE HERE)\n",
    "df_store_2_item_28.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef627d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2_item_28_time = df_store_2_item_28.copy()\n",
    "df_store_2_item_28_time['date'] = pd.to_datetime(df_store_2_item_28_time['date'])\n",
    "df_store_2_item_28_time = df_store_2_item_28_time.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d486cd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify if there are null values\n",
    "df_store_2_item_28.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22496018",
   "metadata": {},
   "source": [
    "## STEP 1 - Identify\n",
    "\n",
    "Following the schema above in this step we use tools to identify characteristics of the time series so we can build an appropriate model. \n",
    "\n",
    "Here we search for answers for questions such as:\n",
    "\n",
    "* _Is the time series stationary?_\n",
    "    \n",
    "* _If not stationary, which transformation should we apply to make it stationary?_\n",
    "   \n",
    "* _Is the time series seasonal?_\n",
    "\n",
    "* _If seasonal what is the seasonal period?_\n",
    "\n",
    "* _Which orders to use? (p for AR, q for MA)_\n",
    "\n",
    "### Visualize times series sales of item 28 at store 2\n",
    "\n",
    "As we saw previously in this data, we have 500 time series which are defined by the pair `store-item`. Here we are working with forecasting individual time series. To forecast sales for all stores and all items we need to apply a forecast model to each one of the time series. \n",
    "\n",
    "With purpose of demonstration, in this tutorial we will work with only one time series: sales of item 28 (the most sold item) at store 2 (the store with the highest number of sales).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af85225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the entire time series diet and show gridlines\n",
    "df_store_2_item_28_time.plot(grid=True,figsize=(20,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af410d1",
   "metadata": {},
   "source": [
    "What can you observe here about seasonality? Is it also possible to already say something about trend?\n",
    "\n",
    "Let's obtain decompose this TS and observe its components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964a1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# TIP: cycle repeats 365 days, i.e., every year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ead340",
   "metadata": {},
   "source": [
    "**From the decomposition what can you conclude about trend and seasonality?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52971735",
   "metadata": {},
   "source": [
    "**Answer**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8e52e4",
   "metadata": {},
   "source": [
    "Since our data is not stationary, we need to answer `What differencing will make it stationary? `. For this we will use our `obtain_adf_kpss_results` function to find out how many times we need to apply differencing in order to make this time series stationary. This will be our parameter `d` for the ARIMA model.\n",
    "\n",
    "### Apply Stationarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836922a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97462c7c",
   "metadata": {},
   "source": [
    "**How many times we need to apply differencing?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10783e9",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24789bf2",
   "metadata": {},
   "source": [
    "### Plot ACF and PACF\n",
    "\n",
    "Autocorrelation and Partial autocorrelation plots are heavily used in time series analysis and forecasting. They can give clues about promising values of ARIMA parameters. It also can shows us if we need to apply differencing of if we have applied it too much.\n",
    "\n",
    "`AutoCorrelation Function - ACF` is the plot of the autocorrelation of a time series by lag. This plot is sometimes called a correlogram. It includes direct and indirect dependence information. In simple terms, ACF describes how well the present value of the series is related with its past values. The bars of the ACF plot represent the ACF values at increasing lags. The blue shaded area represents the confidence interval, which is set to 95% by default. If the bars lie inside the blue shaded region, then they are not statistically significant. \n",
    "\n",
    "Different from ACF, `Partial AutoCorrelation Function - PACF` only describes the direct relationship between an observation and its lag. Basically, instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)).\n",
    "\n",
    "So how these plots can help us finding **p** (AR term) and **q** (MA term)?\n",
    "\n",
    "A good intuition on how these plots relates with parameters of ARIMA is given [here](https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/). You can read more on how to interpret these plots [here](https://people.duke.edu/~rnau/411arim3.htm).\n",
    "\n",
    "As said earlier, ACF and PACF can also give tips about differencing. However, the interpretation can be difficult and sometimes dubious. For this reason, here we will make use of the tests applied above and also, in future section, let [pmdarima](https://alkaline-ml.com/pmdarima/tips_and_tricks.html) works its magic, estimate the appropriate **d** value, and difference the time series accordingly.\n",
    "\n",
    "Therefore, here we will use only the numerical method above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56eef3",
   "metadata": {},
   "source": [
    "## STEP 2: Estimate Coefficients (p,q)\n",
    "\n",
    "Although we had clear signs that we need a SARIMA model, we will start by applying a ARIMA model instead. By doing so we can obtain a better understanding on the differences between ARIMA and SARIMA in what concern the use of Box-Jenkins’s method. In addition, it will be clear the advantages of choosing the appropriate model.\n",
    "\n",
    "ACF and PACF plots can help us find appropriate values for parameters `p` and `q` . However, the interpretation of these plots is not always clear. To obtain more assurance to our choices we can apply an empirical method. This method consists on fitting the ARIMA model for different values of p and q, and choosing the best value based on metrics such as AIC and BIC.\n",
    "\n",
    "`AIC (Akaike Information Criterion)` is a metric which tells us how good a model is. Lower the value, better the model. The AIC also penalizes models which have lots of parameters. This means if we set the order too high compared to the data, we will get a high AIC value. This stops us overfitting to the training data. \n",
    "\n",
    "`BIC (Bayesian Information Criterion)` is similar to AIC, therefore lower value means a better model. However, BIC penalizes additional model orders more than AIC. As consequence, BIC will sometimes suggest a simpler model. \n",
    "\n",
    "After fitting a model, we can access its summary statistics, and there is where we can find the values of AIC and BIC.\n",
    "\n",
    "Usually there is agreement between AIC and BIC. However, in some cases, the AIC may select a model with higher complexity than the BIC, or vice versa. If there is no agreement choose smaller AIC if you prefer a predictive model. Otherwise, choose smaller BIC for an explanatory model.\n",
    "\n",
    "Check the how to obtain the summary statistics in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ff8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2_item_28_time.index = pd.DatetimeIndex(df_store_2_item_28_time.index.values,\n",
    "                               freq=df_store_2_item_28_time.index.inferred_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05070236",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_store_2_item_28_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57721e48",
   "metadata": {},
   "source": [
    "## First ARIMA model (just an example)\n",
    "\n",
    "Let's apply a first ARIMA model. Consider a non-seasonal ARIMA and parameters, `p=1, q=1, d=1`.\n",
    "\n",
    "For more info: https://www.statsmodels.org/devel/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3876a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# just an example\n",
    "model = SARIMAX(df_store_2_item_28_time, \n",
    "                order=#YOUR CODE HERE, \n",
    "                freq='D')\n",
    "# fit model\n",
    "results = model.fit()\n",
    "\n",
    "# statistics of the model\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3888289",
   "metadata": {},
   "source": [
    "Now let's scan different values of p and q and choose the values that points to smaller AIC and/or BIC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e452a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty list to store search results\n",
    "order_aic_bic=[]\n",
    "\n",
    "# Loop over p values from 0-6\n",
    "for p in range(7):\n",
    "  # Loop over q values from 0-6\n",
    "    for q in range(7):\n",
    "      \t# create and fit ARMA(p,q) model\n",
    "        model = SARIMAX(df_store_2_item_28_time, order=(p,1,q), freq=\"D\") #because adf test showed that d=1\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Append order and results tuple\n",
    "        order_aic_bic.append((p,q,results.aic, results.bic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18602014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct DataFrame from order_aic_bic\n",
    "order_df = pd.DataFrame(order_aic_bic, \n",
    "                        columns=['p','q','AIC','BIC'])\n",
    "\n",
    "# Print order_df in order of increasing AIC\n",
    "order_df.sort_values('AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9998d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print order_df in order of increasing BIC\n",
    "order_df.sort_values('BIC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54b7693",
   "metadata": {},
   "source": [
    "* **Which model would you choose if you want a simple model?**\n",
    "* **And if you prefer a preditive model?**\n",
    "\n",
    "Use the preditive one for the following step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bedc6ce",
   "metadata": {},
   "source": [
    "## STEP 3: Model Evaluation\n",
    "\n",
    "Before, using a model we want to know how accurate it is. Here we present some tools to evaluate the model before considering it the best one and putting it to production.\n",
    "\n",
    "For this evaluation we focus on the residuals. The residuals are the difference between the model's one-step-ahead predictions and the real values of the time series. \n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "We start by calculating the `Mean Absolute Error (MAE)` of the residuals. This will show us how far, on average, the predictions are from the true values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_model = SARIMAX(df_store_2_item_28_time, \n",
    "                      order=#YOUR CODE HERE)\n",
    "# fit model\n",
    "arima_results = arima_model.fit()\n",
    "\n",
    "# Calculate the mean absolute error from residuals\n",
    "mae = np.mean(np.abs(arima_results.resid))\n",
    "\n",
    "# Print mean absolute error\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca99c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain MAE using statsmodel\n",
    "results.mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain MSE (Mean Squared Error) using statsmodel\n",
    "results.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc48e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rmse\n",
    "np.sqrt(results.mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50839237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_store_2_item_28_time.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd1504f",
   "metadata": {},
   "source": [
    "**How would you interpret the MAE?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f80f86",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88662032",
   "metadata": {},
   "source": [
    "### Diagnostic Summary Statistics\n",
    "\n",
    "The summary attribute of the fitted model gives us a significant amount of information. However, let's focus our attention on the table of coefficients (below summary). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c0d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18d23ae",
   "metadata": {},
   "source": [
    "In the table of coefficients below, **coef** column shows the weight (i.e. importance) of each feature and how each one impacts the time series. While the **P>|z|** column informs us of the significance of each feature weight. Each weight has a p-value lower or close to 0.05, so it is reasonable to retain all of them in our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results.summary().tables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c28fc8",
   "metadata": {},
   "source": [
    "### Plot Diagnostics\n",
    "\n",
    "In addition, it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. The `plot_diagnostics` object allows us to quickly generate model diagnostics and investigate for any unusual behavior.\n",
    "\n",
    "It shows 4 common plots that help us deciding whether a model is a good fit for the data in question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31a18b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply plot_diagnostic on the results of the model\n",
    "# Create the 4 diagostics plots using plot_diagnostics method\n",
    "\n",
    "#YOUR CODE HERE\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce0ec4",
   "metadata": {},
   "source": [
    "For an ideal model the residuals should be uncorrelated white Gaussian noise centered on zero. By analyzing the plots above having this in mind we can evaluate if we have a good model or not.\n",
    "\n",
    "**How you analyze each one of the plots? (Clockwise from left-top plot):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468d0bd",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "* **Standardized residual**: \n",
    "\n",
    "* **Histogram plus kde estimate**: \n",
    "\n",
    "* **Correlogram** or ACF plot: \n",
    "\n",
    "* **Normal Q-Q:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e635e3",
   "metadata": {},
   "source": [
    "**Is this a good model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c867d2",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679f64b6",
   "metadata": {},
   "source": [
    "**Final tips:**\n",
    "\n",
    "If the residuals are not normally distributed try to increase `d`.\n",
    "\n",
    "If the residuals are correlated try to increase `p` or `q`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11ded40",
   "metadata": {},
   "source": [
    "# SARIMA model\n",
    "\n",
    "Let's consider now seasonality. For this we will need to choose also the seasonal parameters P, Q, D, and m.\n",
    "\n",
    "## Seasonal Differencing\n",
    "\n",
    "We have also learnt that in order to make a time series stationary we may use differencing and that's how we determine the parameter `d` of ARIMA, together with ADF and KPSS test.\n",
    "\n",
    "For a seasonal time series, we may need to apply seasonal differencing. In seasonal differencing, instead of subtracting the most recent time series value, we subtract the time series value from one cycle ago. Therefore, if the time series shows a trend, then we take the normal difference. If there is a strong seasonal cycle, then we will also take the seasonal difference.\n",
    "\n",
    "**D** can be estimated via Canova-Hansen test (This test can be found in de pmdarima package). `auto_arima` will estimate it if `seasonal=True`.\n",
    "\n",
    "Below we apply Canova-Hansen test, as in the [documentation of pmdarima](https://alkaline-ml.com/pmdarima/tips_and_tricks.html#estimating-the-seasonal-differencing-term-d).\n",
    "\n",
    "**1. Observing our EDA you can have an idea of the value of `m`. Which value do you think we should use here?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c769a",
   "metadata": {},
   "source": [
    "**Answer:** m="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35829a18",
   "metadata": {},
   "source": [
    "**2. Use the code below to find out D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f32e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pmdarima.arima.utils import nsdiffs\n",
    "\n",
    "# estimate number of seasonal differences using a Canova-Hansen test\n",
    "D = nsdiffs(df_store_2_item_28_time,\n",
    "            m=7,  # commonly requires knowledge of dataset\n",
    "            max_D=12,\n",
    "            test='ch')  # -> 0\n",
    "\n",
    "print(\"D =\", D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a033a64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use the OCSB test (by default)\n",
    "nsdiffs(df_store_2_item_28_time,\n",
    "        m=7,\n",
    "        max_D=12,\n",
    "        test='ocsb')  # -> 0\n",
    "\n",
    "print(\"D =\", D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e4b318",
   "metadata": {},
   "source": [
    "By the decomposition we've already a feeling of strong seasonality which is confirmed by Canova-Hansen test. \n",
    "\n",
    "A rule of thumb is that d + D should not be greater than 2.\n",
    "\n",
    "There are many guidelines and best practices to achieve this goal, yet the correct parametrization of ARIMA models can be a painstaking manual process that requires domain expertise and time. For example, you can make use of ACF and PACF plots that as said before not always direct to interpret.\n",
    "\n",
    "## Automated Model Selection\n",
    "\n",
    "[`pmdarima`](http://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.auto_arima.html) allows us to automate the search of model orders. We use information we got from the Box-Jenkins identification step to predefine some of the orders before we fit. Automated Model Selection can speed up the process of choosing model orders, but needs to be done with care. Automation can make mistakes since the input data can be imperfect and affect the test scores in non-predictable ways.\n",
    "\n",
    "The only non-optional parameter in `auto_arima` is data. However, using your knowledge to specify other parameters can help finding the best model. So let's use what we know so far to try to find our best model.\n",
    "\n",
    "**With the information you got so far what are the values of the parameter below?**\n",
    "\n",
    "* From ADF and KPSS tests: d = ?\n",
    "* From Canova-Hansen test: D = ?\n",
    "* The EDA showed week seasonality: m = ?\n",
    "\n",
    "Here, we will opt for using the automated model selection provided by pmdarima. R also provides an [ARIMA automated method](https://otexts.com/fpp2/arima-r.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacea499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store_2_item_28_time.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba358349",
   "metadata": {},
   "source": [
    "**Fill the values of m, d, and D in the code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ea90a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pmdarima as pm\n",
    "\n",
    "# Create auto_arima model\n",
    "model1 = pm.auto_arima(df_store_2_item_28_time, #time series\n",
    "                      seasonal=True, # is the time series seasonal\n",
    "                      m=#YOUR CODE HERE, # the seasonal period - one week?\n",
    "                      d=#YOUR CODE HERE, # non-seasonal difference order\n",
    "                      D=#YOUR CODE HERE, # seasonal difference order\n",
    "                 \t  max_p=6, # max value of p to test \n",
    "                      max_q=6, # max value of p to test\n",
    "                      max_P=6, # max value of P to test \n",
    "                      max_Q=6, # max value of Q to test \n",
    "                      information_criterion='aic', # used to select best mode\n",
    "                      trace=True, # prints the information_criterion for each model it fits\n",
    "                      error_action='ignore', # ignore orders that don't work\n",
    "                      stepwise=True, # apply an intelligent order search\n",
    "                      suppress_warnings=True) \n",
    "\n",
    "# Print model summary\n",
    "print(model1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f96b47",
   "metadata": {},
   "source": [
    "**Which model the Automated model selection suggests?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5696ba4",
   "metadata": {},
   "source": [
    " \n",
    "It suggests SARIMA$???$ as best model based on AIC. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6637c3",
   "metadata": {},
   "source": [
    "## Fitting and Evaluating Best Model\n",
    "\n",
    "**Fit the previous model and evaluate it. To evaluate calculate MAE, observe the summary statistics, and plot to verify the normality of not of residuals.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78e209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "sarima_01_model = SARIMAX(df_store_2_item_28_time, \n",
    "                          order=#YOUR CODE HERE, \n",
    "                          seasonal_order=#YOUR CODE HERE)\n",
    "# fit\n",
    "sarima_01_results = sarima_01_model.fit()\n",
    "\n",
    "# Calculate the mean absolute error from residuals\n",
    "mae = np.mean(np.abs(sarima_01_results.resid))\n",
    "\n",
    "# Print mean absolute error\n",
    "print('MAE: %.3f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e77609",
   "metadata": {},
   "source": [
    "1. MAE of this Seasonal ARIMA is better than the Non-seasonal ARIMA which is expected since our data is clearly seasonal and this cannot be ignored.\n",
    "\n",
    "2. Is this a good model? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061c68f3",
   "metadata": {},
   "source": [
    "P > |z| shows that all coefficients are significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1f62e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 4 diagnostics plots\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20793ef3",
   "metadata": {},
   "source": [
    "**Your interpretation:**\n",
    "\n",
    "* **Standardized residual**: \n",
    "\n",
    "* **Histogram plus kde estimate**: \n",
    "\n",
    "* **Correlogram** or ACF plot: \n",
    "\n",
    "* **Normal Q-Q:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d86420",
   "metadata": {},
   "source": [
    "**Is this a good model?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795f5863",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d090d2ff",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "## SARIMA vs ARIMA forecasts\n",
    "\n",
    "We will continue using both ARIMA and SARIMA models even if we know that SARIMA in this case is the most adequate model. The goal here is to show why SARIMA is the most adequate.\n",
    "\n",
    "### Forecast in Sample\n",
    "\n",
    "To have a feeling on how good the chosen models are doing we will take the last 90 days in training dataset as validation data.\n",
    "\n",
    "### Metrics Used to Compare Models\n",
    "\n",
    "The models presented here and in the [next notebook](https://github.com/MKB-Datalab/time-series-analysis-with-SARIMAX-and-Prophet/blob/master/notebooks/03-Forecasting_with_Facebook_Prophet.ipynb) will be evaluated using [`MAE (Mean Absolute Error)`]( https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error) and [`MAPE (Mean Absolute Percentage Error)`]( https://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-percentage-error). These are popular metrics when evaluating regression models and you will come often across [MAE and MAPE when evaluating forecasting models]( https://otexts.com/fpp2/accuracy.html). \n",
    "\n",
    "When comparing forecast methods applied to a single time series, or to several time series with the same units, `MAE` is popular as it is easy to both understand and compute. Percentage errors measures such as `MAPE` have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b53c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9a1907",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sarima_01_results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6056e90",
   "metadata": {},
   "source": [
    "For our forecasting in sample we use the method `get_prediction` using the last 90 days of the training data as validation data. After that we use `mean_absolute_error` and `mean_absolute_percentage_error` from `sklearn.metrics` to obtain `MAE` and `MAPE` for all three models.\n",
    "\n",
    "Observe that to use the last 90 days as validation we make `start=-90` in `get_prediction()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27978cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ARIMA mean forecast\n",
    "arima_pred = arima_results.get_prediction(start=-90, \n",
    "                                          dynamic=True)\n",
    "arima_mean = arima_pred.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2645b259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARIMA mean forecast\n",
    "sarima_01_pred = # YOUR CODE HERE\n",
    "sarima_01_mean = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77a6003",
   "metadata": {},
   "source": [
    "**Calculate `mean_absolute_error` and `mean_absolute_percentage_error` using `sklearn.metricss`**\n",
    "\n",
    "**TIP: use last 90 of `df_store_2_item_28_time` as actual data and the predicted mean as predicted data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba6489",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "metrics_arima = [# YOUR CODE HERE - MAE, \n",
    "                 # YOUR CODE HERE- MAPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3f2b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_sarima_01 = [# YOUR CODE HERE - MAE, \n",
    "                 # YOUR CODE HERE- MAPE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37e7b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Present results in a dataframe\n",
    "df_arima_results = pd.DataFrame({'metrics':['MAE','MAPE'],\n",
    "              'ARIMA(6,1,6)':metrics_arima, \n",
    "              'SARIMA(6,1,1)(6,1,0)7':metrics_sarima_01,\n",
    "             })\n",
    "\n",
    "df_arima_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5f318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results in csv (results_arima.csv)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083469c7",
   "metadata": {},
   "source": [
    "The second SARIMA model, the one pointed out by the automated model selection shows the best values for the metrics considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd84d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_mean.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf98c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_store_2_item_28_time.index\n",
    "# Plot mean ARIMA and SARIMA predictions and observed\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Comparing forecasting in sample of all models', size = 16)\n",
    "plt.plot(arima_mean.index, arima_mean, label='ARIMA(6,1,6)')\n",
    "plt.plot(sarima_01_mean.index, sarima_01_mean, label='SARIMAX(6,1,1)(6,1,0)7')\n",
    "plt.plot(df_store_2_item_28_time[-90:], label='observed')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d30526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Forecasting in sample - Observed values vs ARIMA(6,1,6)', size = 16)\n",
    "plt.plot(df_store_2_item_28_time[-90:], label='observed', color='red')\n",
    "plt.plot(arima_mean.index, arima_mean, label='ARIMA(6,1,6)', color='blue')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbf0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Forecasting in sample - Observed values vs SARIMA(6,1,1)(6,1,0)7', size = 16)\n",
    "plt.plot(df_store_2_item_28_time[-90:], label='observed', color='red')\n",
    "plt.plot(sarima_01_mean.index, sarima_01_mean, label='SARIMA(6,1,1)(6,1,0)7', color='green')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0211b4",
   "metadata": {},
   "source": [
    "**How you'd interpret the plots above?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534960d5",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105f2f99",
   "metadata": {},
   "source": [
    "### Forecast Out of Sample\n",
    "\n",
    "Let's predict 90 days ahead. For this part we will just use the ARIMA model (ARIMAX(5,1,6)) and the SARIMA model chosen by automated model selection: SARIMA(6,1,1)x(6,1,0)7.\n",
    "\n",
    "Notice that now we use `get_forecast` in place of `get_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ARIMA mean forecast\n",
    "arima_pred = arima_results.get_forecast(steps=90)\n",
    "arima_mean = arima_pred.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA predictions\n",
    "arima_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SARIMA mean forecast\n",
    "sarima_01_pred = # YOUR CODE HERE\n",
    "sarima_01_mean = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0820de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA predictions\n",
    "sarima_01_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9f8bf",
   "metadata": {},
   "source": [
    "Notice that we are covering the interval in our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c110355b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = df_store_2_item_28_time.index\n",
    "# Plot mean ARIMA and SARIMA predictions and observed\n",
    "plt.title(\"Comparing Forecasting 90 days ahead - ARIMA vs SARIMA\", size =16)\n",
    "plt.plot(df_store_2_item_28_time['2017':], label='observed')\n",
    "plt.plot(arima_mean.index, arima_mean, label='ARIMA(6,1,6)')\n",
    "plt.plot(sarima_01_mean.index, sarima_01_mean, label='SARIMA(6,1,1)(6,1,0)7')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d7502",
   "metadata": {},
   "source": [
    "**Give an interpretation for the plot above**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33505836",
   "metadata": {},
   "source": [
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fa9621",
   "metadata": {},
   "source": [
    "# Saving Model\n",
    "\n",
    "Once the best model is found, it is useful to know how to save it. For this we use the `joblib` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3870906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import joblib\n",
    "import joblib\n",
    "\n",
    "# Set model name\n",
    "filename = \"../model/store_2_item_28_model.pkl\"\n",
    "\n",
    "# Pickle it\n",
    "joblib.dump(sarima_01_model,filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05e9ad",
   "metadata": {},
   "source": [
    "To load the saved model use `.load()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../model/store_2_item_28_model.pkl\"\n",
    "loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44de9d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f8b70",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this section we introduced ARIMA models and its variants: Seasonal ARIMA (SARIMA) and ARIMAX which uses external data (exogenous inputs) to improve the performance of the ARIMA model. \n",
    "\n",
    "We followed the Box-Jenkins method to find the best model considering a part of our dataset (time series of sales of product 28 of Walmart's store 2). As first step we've identified important characteristics of our time series such as stationarity and seasonality. \n",
    "\n",
    "Then, we also used graphical and statistical methods such as follows to find the best fit model:\n",
    "* [Augmented Dickey-Fuller test](https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test),\n",
    "* [Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test](https://en.wikipedia.org/wiki/KPSS_test),\n",
    "* [Canova-Hansen test](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.arima.nsdiffs.html#pmdarima.arima.nsdiffs), \n",
    "* Exploring model summary statistics,\n",
    "* Analyze plots obtained using the statsmodel method `plot_diagnostics`.  \n",
    "\n",
    "Once we've found a model considered good, we used it to forecast in sample, i.e., we applied the model on part of the training data as validation data. Like this, we were able to have a feeling of how good the model is. After that we forecast out of the sample, i.e., 90 days in future. \n",
    "\n",
    "Both the application of the [Box-Jenkins Method](https://en.wikipedia.org/wiki/Box%E2%80%93Jenkins_method) as well as using the chosen model to forecast was applied on ARIMA and SARIMA models.  \n",
    "\n",
    "We knew since the beginning that the most appropriate model would be a SARIMA model since we were dealing with a seasonal time series. However, working with both models gave us the opportunity to see the nuances in the application of the Box-Jenkins for these two types of models. Moreover, we could see clearly that if seasonality is not considered we are not using all information and therefore not making the best predictions possible. This became clear also from the forecasting plots.\n",
    "\n",
    "We didn't provide an explicit example of (S)ARIMAX, i.e., an (S)ARIMA model using external data. Therefore, I suggest trying to improve the best model by adding holidays as external data. [Here]( https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html) you have an example using exogenous regressor. The [`holidays` Python library](https://pypi.org/project/holidays/) helps you obtaining holidays.\n",
    "\n",
    "Next, in notebook [03-Forecasting_with_Facebook_Prophet_v220423.ipynb](https://github.com/MKB-Datalab/workshop_ts_forecasting/notebooks/03-Forecasting_with_Facebook_Prophet_v220423.ipynb) you are introduce to Facebook Prophet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
